import numpy as np

# 1) Generate a set of 100 data points by sampling the function LaTeX: h\left(x\right)=0.5+0.4\cos\left(2.5\pi x\right) with added uniform noise in the interval [-0.1, 0.1] and with x values taken randomly
#  from a uniform distribution in the interval [0.0, 1.0].

# 2) Determine Gaussian centers by the K-means algorithm, and set the variance of each cluster according to the variance of the cluster. If a cluster contains only one sample point, use as its variance the mean 
# variance of all the other clusters. 

# 3) 3) Implement the second layer of the RBF to predict the output from the projected input (through the basis functions)

# Option a) Implement the LMS rule for weight update (note a bias term is needed).  Do this in vector notation (using, e.g. numpy).  [Full credit, 3 points off for not using vectors, calculating each one by hand]

# 4) Perform the following:

# Vary the number of bases in the range of 3, 5, 10, and 15.
# Use two values of : 0.01 and 0.02.

# For each of the above 8 cases, stop training after 100 epochs. Plot for each case a graph that shows the data points, the original function where the data points are sampled from, and the function generated by the RBF network.

# Comment on the dependence of the network performance on the number of bases. Also comment on the choice of .

# In addition, repeat the above implementation but use the same variance for all clusters as described in the lectures. Comment on the comparative advantages of the two methods of determining cluster variance.

class RBF:

    def __init__(self, bases, nu, epochs=100):
        self.bases = bases
        self.nu = nu
        self.epochs = epochs
        noise = np.random.uniform(low=-0.1, high=0.1, size=(100, 1))
        initial_x_vals = np.random.uniform(low=0, high=1.0, size=(100, 1))
        inputs = []
        for i in range(0, 100):
            x_val = 0.5 + 0.4 * np.cos(2.5 * np.pi * initial_x_vals[i]) + noise[i]
            inputs.append(x_val[0])
        self.inputs = inputs
        

        self.weights = np.random.uniform(low=0, high=1.0, size=(100, 1))
        self.biases = np.random.uniform(low=0, high=1.0, size=(100, 1))

        def gaussian_rbf(self, x, mean, standard_deviation):
            return np.exp((-(x-mean)**2) / (2 * standard_deviation**2))

        def k_means(self):
            # Generate random clusters from points
            current_clusters = np.random.choice(np.squeeze(self.inputs, size=self.bases))
            updated_clusters = current_clusters.copy()
            # Initialize standard deviations to 0
            variance = np.zeros(self.bases)

            converged = False

            while not converged:
                empty_or_single_point_clusters = []
                distance_to_clusters = np.squeeze(np.abs(self.inputs[:, np.newaxis] - current_clusters[np.newaxis, :]))

                closest_cluster_to_points = np.argmin(distance_to_clusters, axis=1)

                current_clusters, points_in_cluster = update_clusters(closest_cluster_to_points, current_clusters, self.bases)
                converged = np.linalg.norm(current_clusters - current_clusters) < 0.000001

                updated_clusters = current_clusters.copy()

                distances = np.squeeze(np.abs(self.inputs[:, np.newaxis] - current_clusters[np.newaxis, :]))
                closest_cluster_to_points = np.argmin(distances, axis=1)

                for i in range(self.bases):
                    points_in_cluster = self.inputs[closest_cluster_to_points == i]
                    if len(points_in_cluster) < 2:
                        empty_or_single_point_clusters.append(i)
                    else:
                        variance[i] = np.std(self.inputs[closest_cluster_to_points == i]) **2

                if len(empty_or_single_point_clusters) > 1:
                    mean_variance_pts = []
                    for i in range(self.bases):
                        if i not in empty_or_single_point_clusters:
                            mean_variance_pts.append(self.inputs[closest_cluster_to_points == i])
                    mean_variance_pts = np.concatenate(mean_variance_pts).ravel()
                    variance[empty_or_single_point_clusters] = np.mean(np.std(mean_variance_pts)**2)

            return current_clusters, variance
                

        def update_clusters(closest_cluster_to_points, initial_clusters):
            for i in range(0, self.bases):
                points_in_cluster = self.inputs[closest_cluster_to_points == i]
                if len(points_in_cluster) > 0:
                    initial_clusters[i] = np.mean(points_in_cluster, axis=0)
            return initial_clusters, points_in_cluster

        def train(self, targets):
            self.gaussian_centers, self.variance = self.kmeans()
            
            for epoch in range(self.epochs):
                for i in range(inputs.shape[0]):
                    for center, std in zip(self.gaussian_centers, self.variance):
                        gaussian = np.array([self.gaussian_rbf(self.inputs[i], center, std)])
                        forward_pass = gaussian.T.dot(self.weights) +self.biases

                        loss = self.loss_function(targets[i], forward_pass)

                        backward_prop = -(targets[i] - forward_pass).flatten()

                        self.updates(forward_pass, backward_prop)


        def loss_function(self, y, f):
            return (y - f).flatten() ** 2

        def updates(self, f, bp):
            self.weights = self.weights - self.nu * f * bp
            self.biases = self.biases - self.nu * bp

        def step_function(self):
            predictions = []
            for i in range(self.inputs.shape[0]):
                    for center, std in zip(self.gaussian_centers, self.variance):
                        gaussian = np.array([self.gaussian_rbf(self.inputs[i], center, std)])
                        forward_pass = gaussian.T.dot(self.weights) +self.biases
                        predictions.append(forward_pass)
            return predictions




rbf = RBF(3)