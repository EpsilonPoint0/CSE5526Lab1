from re import I
import numpy as np
import matplotlib as plt
import matplotlib.pyplot as mp
from sklearn.cluster import k_means
from sklearn.linear_model import LinearRegression

# 1) Generate a set of 100 data points by sampling the function LaTeX: h\left(x\right)=0.5+0.4\cos\left(2.5\pi x\right) with added uniform noise in the interval [-0.1, 0.1] and with x values taken randomly
#  from a uniform distribution in the interval [0.0, 1.0].

# 2) Determine Gaussian centers by the K-means algorithm, and set the variance of each cluster according to the variance of the cluster. If a cluster contains only one sample point, use as its variance the mean 
# variance of all the other clusters. 

# 3) 3) Implement the second layer of the RBF to predict the output from the projected input (through the basis functions)

# Option a) Implement the LMS rule for weight update (note a bias term is needed).  Do this in vector notation (using, e.g. numpy).  [Full credit, 3 points off for not using vectors, calculating each one by hand]

# 4) Perform the following:

# Vary the number of bases in the range of 3, 5, 10, and 15.
# Use two values of : 0.01 and 0.02.

# For each of the above 8 cases, stop training after 100 epochs. Plot for each case a graph that shows the data points, the original function where the data points are sampled from, and the function generated by the RBF network.

# Comment on the dependence of the network performance on the number of bases. Also comment on the choice of .

# In addition, repeat the above implementation but use the same variance for all clusters as described in the lectures. Comment on the comparative advantages of the two methods of determining cluster variance.

class RBF:

    def __init__(self, bases, nu, epochs=100, kmeans=True):
        self.bases = bases
        self.nu = nu
        self.epochs = epochs
        self.kmeans =kmeans
        self.variance = []
        self.inputs = np.random.uniform(low=0.0, high=1, size=(100, 1))
        noise = np.random.uniform(low=-0.1, high=0.1, size=(100, 1))
        initial_targets = np.random.uniform(low=0, high=1.0, size=(100, 1))
        targets = []
        #generate data from function
        for i in range(len(self.inputs)):
            
            x_val = 0.5 + 0.4 * np.cos(2.5 * np.pi * self.inputs[i][0]) + noise[i]
            targets.append(x_val[0])
        self.targets = targets
        

        self.weights = np.random.uniform(low=0, high=1.0, size=bases)
        self.biases = np.random.uniform(low=0, high=1.0, size=1)

    def gaussian_rbf(self, x, mean, standard_deviation):
        return np.exp((-1 / (2 * standard_deviation** 2)) * (x - mean) ** 2)

    def train(self, num_clusters):
        if(self.kmeans):
            self.gaussian_centers, labels, inertia = k_means(self.inputs, num_clusters)

            for i in range(len(self.gaussian_centers)):
                self.variance.append(np.sqrt((1/self.gaussian_centers[i]) * inertia))
            #print(self.gaussian_centers)
        else:
            self.gaussian_centers, labels, inertia = k_means(self.inputs, num_clusters)
            sum = 0
            for i in range(len(self.gaussian_centers)):
                self.variance.append((1/self.gaussian_centers[i]) * inertia)
                sum = sum + self.variance[i]
            average_variance = sum / len(self.variance)

            for i in range(len(self.variance)):
                self.variance[i] = average_variance

            
        #mp.plot(self.gaussian_rbf(np.linspace(0, 1, 100), self.gaussian_centers[0], self.variance[0]))


        for epoch in range(self.epochs):
             for i in range(self.inputs.shape[0]):
               
                 gaussian = np.empty(self.bases)

                 for center, var, in zip(self.gaussian_centers, self.variance):
                     np.append(gaussian, self.gaussian_rbf(self.inputs[i], center, var))
                 #print(self.gaussian_centers)
                
                 forward_pass = gaussian.T.dot(self.weights) +self.biases

                 loss = self.loss_function(self.targets[i], forward_pass)

                 backward_prop = -(self.targets[i] - forward_pass).flatten()

                 self.updates(forward_pass, backward_prop)

        #print(self.weights)
   

    def loss_function(self, target, true):
        return (target - true).flatten() ** 2

    def updates(self, f, bp):
        self.weights = self.weights - self.nu * f * bp
        #self.biases = self.biases - self.nu * bp

    def linear_regression(self):
        self.predictions = []
        for i in range(self.inputs.shape[0]):
                gaussian = np.empty(self.bases)
                for center, var, in zip(self.gaussian_centers, self.variance):
                    np.append(gaussian, self.gaussian_rbf(self.inputs[i], center, var))
                #print(gaussian)
                forward_pass = gaussian.T.dot(self.weights) +self.biases
                self.predictions.append(forward_pass) 
        return self.predictions

    def plot_data(self):
        x = np.linspace(0,1,100)
        y = 0.5 + 0.4*np.cos(2.5 * np.pi * x)
        mp.plot(x, y)

    def error_calculation(self):
        error = []
        sum = 0
        for i in range(len(self.inputs)):
            err = self.targets[i] - self.predictions[i]
            error.append(err)
            sum = sum + err
        return sum / 100
        
            



rbf = RBF(nu=0.01, bases=3, kmeans=True)
rbf.train(3)
output = rbf.linear_regression()
# print(predictions)
mp.scatter(rbf.inputs, rbf.targets, label='true')
error_sum = rbf.error_calculation()
print(error_sum)
#model.fit(rbf.weights, rbf.gaussian_centers)
mp.scatter(rbf.inputs, output, label='RBF')
rbf.plot_data()
mp.legend()
mp.show()